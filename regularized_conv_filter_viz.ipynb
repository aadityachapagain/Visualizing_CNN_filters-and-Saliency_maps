{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regularized_conv_filter_viz",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UZ6_7qBUjCWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a836333-75c4-4e3b-a8d5-6fd2753ee0d2"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image as pil_image\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras import layers\n",
        "from keras.applications import vgg16\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8q41DackXt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LAFr66b-pr5L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import ZeroPadding2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_PHrG4UJjhmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "70d46cc3-8745-4ced-8fae-c07f9816bca5"
      },
      "cell_type": "code",
      "source": [
        "LAYER_NAME = 'block5_conv1'\n",
        "\n",
        "# build the VGG16 network with ImageNet weights\n",
        "vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "vgg.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model loaded.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EEOt0D3OkCpf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name].output\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oeBKFRWBkOZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_filters(filters, img_width, img_height):\n",
        "    margin = 5\n",
        "    n = int(len(filters)**0.5)\n",
        "    width = n * img_width + (n - 1) * margin\n",
        "    height = n * img_height + (n - 1) * margin\n",
        "    stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "    # fill the picture with our saved filters\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            index = i * n + j\n",
        "            if index < len(filters):\n",
        "                img = filters[i * n + j]\n",
        "                stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
        "                                 (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
        "\n",
        "    # save the result to disk\n",
        "    cv2.imwrite('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18daTjJukc-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def deprocess_image(x):\n",
        "    \"\"\"function to convert a float array into a valid uint8 image.\n",
        "        x: A numpy-array representing the generated image.\n",
        "        return: A processed numpy-array, which could be used in e.g. imshow.\n",
        "    \"\"\"\n",
        "    # normalize tensor: center on 0., ensure std is 0.25\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + K.epsilon())\n",
        "    x *= 0.25\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"function to normalize a tensor.\n",
        "    x: An input tensor.\n",
        "    return: The normalized input tensor.\n",
        "    \"\"\"\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5CcUaAkC2RL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_image(x, former):\n",
        "    \"\"\"function to convert a valid uint8 image back into a float array.\n",
        "       Reverses `deprocess_image`.\n",
        "        x: A numpy-array, which could be used in e.g. imshow.\n",
        "        former: The former numpy-array. Need to determine the former mean and variance.\n",
        "        return: A processed numpy-array representing the generated image.\n",
        "    \"\"\"\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((2, 0, 1))\n",
        "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ppk_HgugkgxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define regularizations:\n",
        "def blur_regularization(img, grads, size = (3, 3)):\n",
        "    return cv2.blur(img, size)\n",
        "\n",
        "def decay_regularization(img, grads, decay = 0.8):\n",
        "    return decay * img\n",
        "\n",
        "def clip_weak_pixel_regularization(img, grads, percentile = 1):\n",
        "    clipped = img\n",
        "    threshold = np.percentile(np.abs(img), percentile)\n",
        "    clipped[np.where(np.abs(img) < threshold)] = 0\n",
        "    return clipped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUnisKSAkp90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_ascent_iteration(loss_function, img):\n",
        "    loss_value, grads_value = loss_function([img])    \n",
        "    gradient_ascent_step = img + grads_value * 0.9\n",
        "\n",
        "    #Convert to row major format for using opencv routines\n",
        "    grads_row_major = np.transpose(grads_value[0, :], (1, 2, 0))\n",
        "    img_row_major = np.transpose(gradient_ascent_step[0, :], (1, 2, 0))\n",
        "\n",
        "    #List of regularization functions to use\n",
        "    regularizations = [blur_regularization, decay_regularization, clip_weak_pixel_regularization]\n",
        "\n",
        "    #The reguarlization weights\n",
        "    weights = np.float32([3, 3, 1])\n",
        "    weights /= np.sum(weights)\n",
        "\n",
        "    images = [reg_func(img_row_major, grads_row_major) for reg_func in regularizations]\n",
        "    weighted_images = np.float32([w * image for w, image in zip(weights, images)])\n",
        "    img = np.sum(weighted_images, axis = 0)\n",
        "\n",
        "    #Convert image back to 1 x 3 x height x width\n",
        "    img = np.float32([np.transpose(img, (2, 0, 1))])\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MzF55LeQtiSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGVdXe3_kwxn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_filter(input_img, filter_index,layer, number_of_iterations = 20):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
        "    else:\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = K.gradients(loss, input_img)[0]\n",
        "\n",
        "    # normalization trick: we normalize the gradient\n",
        "    grads = normalize(grads)\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_img], [loss, grads])\n",
        "    img = input_img * 1\n",
        "\n",
        "    # we run gradient ascent for 20 steps\n",
        "    for i in range(number_of_iterations):\n",
        "        img = gradient_ascent_iteration(iterate, img)\n",
        "\n",
        "    # decode the resulting input image\n",
        "    img = deprocess_image(img[0])\n",
        "    print (\"Done with filter\", filter_index)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "og2sqVb9waVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_1f5iIoDn1lR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_layer(model, layer_name, step=1., epochs=15, upscaling_steps=9, upscaling_factor=1.2,\n",
        "                    output_dim=(412, 412), filter_range=(0, None)):\n",
        "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
        "    model: The model containing layer_name.\n",
        "    layer_name: The name of the layer to be visualized.\n",
        "                Has to be a part of model.\n",
        "    step: step size for gradient ascent.\n",
        "    epochs: Number of iterations for gradient ascent.\n",
        "    upscaling_steps: Number of upscaling steps.\n",
        "                     Starting image is in this case (80, 80).\n",
        "    upscaling_factor: Factor to which to slowly upgrade\n",
        "                      the image towards output_dim.\n",
        "    output_dim: [img_width, img_height] The output image dimensions.\n",
        "    filter_range: Tupel[lower, upper]\n",
        "                  Determines the to be computed filter numbers.\n",
        "                  If the second value is `None`,\n",
        "                  the last filter will be inferred as the upper boundary.\n",
        "    \"\"\"\n",
        "\n",
        "    def _generate_filter_image(input_img,\n",
        "                               layer_output,\n",
        "                               filter_index):\n",
        "        \"\"\"Generates image for one particular filter.\n",
        "            input_img: The input-image Tensor.\n",
        "            layer_output: The output-image Tensor.\n",
        "            filter_index: The to be processed filter number.\n",
        "                          Assumed to be valid.\n",
        "            return: Either None if no image could be generated.\n",
        "            or a tuple of the image (array) itself and the last loss.\n",
        "        \"\"\"\n",
        "        s_time = time.time()\n",
        "\n",
        "        # we build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
        "        else:\n",
        "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # we compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, input_img)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads = normalize(grads)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([input_img], [loss, grads])\n",
        "\n",
        "        # we start from a gray image with some random noise\n",
        "        intermediate_dim = tuple(\n",
        "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            input_img_data = np.random.random(\n",
        "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
        "        else:\n",
        "            input_img_data = np.random.random(\n",
        "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
        "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
        "\n",
        "        # Slowly upscaling towards the original size prevents\n",
        "        # a dominating high-frequency of the to visualized structure\n",
        "        # as it would occur if we directly compute the 412d-image.\n",
        "        # Behaves as a better starting point for each following dimension\n",
        "        # and therefore avoids poor local minima\n",
        "        for up in reversed(range(upscaling_steps)):\n",
        "            # we run gradient ascent for e.g. 20 steps\n",
        "            for _ in range(epochs):\n",
        "                loss_value, grads_value = iterate([input_img_data])\n",
        "                input_img_data += grads_value * 0.9\n",
        "\n",
        "                # some filters get stuck to 0, we can skip them\n",
        "                if loss_value <= K.epsilon():\n",
        "                    return None\n",
        "                  \n",
        "                grads_row_major = np.transpose(grads_value[0, :], (1, 2, 0))\n",
        "                img_row_major = np.transpose(input_img_data[0, :], (1, 2, 0))\n",
        "\n",
        "                #List of regularization functions to use\n",
        "                regularizations = [blur_regularization, decay_regularization, clip_weak_pixel_regularization]\n",
        "\n",
        "                #The reguarlization weights\n",
        "                weights = np.float32([3, 3, 1])\n",
        "                weights /= np.sum(weights)\n",
        "\n",
        "                images = [reg_func(img_row_major, grads_row_major) for reg_func in regularizations]\n",
        "                weighted_images = np.float32([w * image for w, image in zip(weights, images)])\n",
        "                img = np.sum(weighted_images, axis = 0)\n",
        "\n",
        "                #Convert image back to 1 x 3 x height x width\n",
        "                input_img_data = np.float32([np.transpose(img, (2, 0, 1))])\n",
        "            # Calulate upscaled dimension\n",
        "            intermediate_dim = tuple(\n",
        "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
        "            # Upscale\n",
        "            img = deprocess_image(input_img_data[0])\n",
        "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
        "                                                           pil_image.BICUBIC))\n",
        "            input_img_data = [process_image(img, input_img_data[0])]\n",
        "\n",
        "        # decode the resulting input image\n",
        "        img = deprocess_image(input_img_data[0])\n",
        "        e_time = time.time()\n",
        "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
        "                                                                  loss_value,\n",
        "                                                                  e_time - s_time))\n",
        "        return img, loss_value\n",
        "\n",
        "    def _draw_filters(filters, n=None):\n",
        "        \"\"\"Draw the best filters in a nxn grid.\n",
        "            filters: A List of generated images and their corresponding losses\n",
        "                     for each processed filter.\n",
        "            n: dimension of the grid.\n",
        "               If none, the largest possible square will be used\n",
        "        \"\"\"\n",
        "        if n is None:\n",
        "            n = int(np.floor(np.sqrt(len(filters))))\n",
        "\n",
        "        # the filters that have the highest loss are assumed to be better-looking.\n",
        "        # we will only keep the top n*n filters.\n",
        "        filters.sort(key=lambda x: x[1], reverse=True)\n",
        "        filters = filters[:n * n]\n",
        "\n",
        "        # build a black picture with enough space for\n",
        "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
        "        MARGIN = 5\n",
        "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
        "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
        "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
        "\n",
        "        # fill the picture with our saved filters\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                img, _ = filters[i * n + j]\n",
        "                width_margin = (output_dim[0] + MARGIN) * i\n",
        "                height_margin = (output_dim[1] + MARGIN) * j\n",
        "                stitched_filters[\n",
        "                    width_margin: width_margin + output_dim[0],\n",
        "                    height_margin: height_margin + output_dim[1], :] = img\n",
        "\n",
        "        # save the result to disk\n",
        "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
        "\n",
        "    # this is the placeholder for the input images\n",
        "    # initializing the model with zeros\n",
        "    assert len(model.inputs) == 1\n",
        "    input_img = model.inputs[0]\n",
        "\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "\n",
        "    output_layer = layer_dict[layer_name]\n",
        "    assert isinstance(output_layer, layers.Conv2D)\n",
        "\n",
        "    # Compute to be processed filter range\n",
        "    filter_lower = filter_range[0]\n",
        "    filter_upper = (filter_range[1]\n",
        "                    if filter_range[1] is not None\n",
        "                    else len(output_layer.get_weights()[1]))\n",
        "    assert(filter_lower >= 0\n",
        "           and filter_upper <= len(output_layer.get_weights()[1])\n",
        "           and filter_upper > filter_lower)\n",
        "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
        "\n",
        "    # iterate through each filter and generate its corresponding image\n",
        "    processed_filters = []\n",
        "    for f in range(filter_lower, filter_upper):\n",
        "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
        "\n",
        "        if img_loss is not None:\n",
        "            processed_filters.append(img_loss)\n",
        "\n",
        "    print('{} filter processed.'.format(len(processed_filters)))\n",
        "    # Finally draw and store the best filters to disk\n",
        "    _draw_filters(processed_filters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5lvJjTFx3rbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "cd5ed93a-a424-448f-9040-ef98739a0386"
      },
      "cell_type": "code",
      "source": [
        "LAYER_NAME = 'block5_conv1'\n",
        "\n",
        "# build the VGG16 network with ImageNet weights\n",
        "vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "vgg.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model loaded.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-UE_Mwa63vmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4906
        },
        "outputId": "b5cdf935-4b31-4b6d-d14a-2d5b7c779f75"
      },
      "cell_type": "code",
      "source": [
        "visualize_layer(vgg, LAYER_NAME)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute filters 0 to 512\n",
            "Costs of filter   0:    59 ( 16.67s )\n",
            "Costs of filter   2:    59 ( 5.34s )\n",
            "Costs of filter   3:    52 ( 5.34s )\n",
            "Costs of filter   4:    75 ( 5.34s )\n",
            "Costs of filter   5:    65 ( 5.34s )\n",
            "Costs of filter   7:   137 ( 5.39s )\n",
            "Costs of filter   8:   136 ( 5.46s )\n",
            "Costs of filter   9:    69 ( 5.42s )\n",
            "Costs of filter  10:    75 ( 5.42s )\n",
            "Costs of filter  13:    58 ( 5.42s )\n",
            "Costs of filter  14:   119 ( 5.45s )\n",
            "Costs of filter  15:    73 ( 5.46s )\n",
            "Costs of filter  17:    82 ( 5.43s )\n",
            "Costs of filter  18:   123 ( 5.47s )\n",
            "Costs of filter  20:    57 ( 5.47s )\n",
            "Costs of filter  24:    80 ( 5.51s )\n",
            "Costs of filter  25:   112 ( 5.50s )\n",
            "Costs of filter  28:    96 ( 5.53s )\n",
            "Costs of filter  29:   110 ( 5.58s )\n",
            "Costs of filter  30:    94 ( 5.55s )\n",
            "Costs of filter  31:    66 ( 5.57s )\n",
            "Costs of filter  32:    51 ( 5.55s )\n",
            "Costs of filter  34:    77 ( 5.54s )\n",
            "Costs of filter  37:   128 ( 5.56s )\n",
            "Costs of filter  42:   102 ( 5.62s )\n",
            "Costs of filter  43:    69 ( 5.60s )\n",
            "Costs of filter  44:    83 ( 5.63s )\n",
            "Costs of filter  46:   124 ( 5.64s )\n",
            "Costs of filter  48:   109 ( 5.65s )\n",
            "Costs of filter  49:   118 ( 5.63s )\n",
            "Costs of filter  53:   115 ( 5.65s )\n",
            "Costs of filter  55:    65 ( 5.69s )\n",
            "Costs of filter  56:   115 ( 5.67s )\n",
            "Costs of filter  58:    50 ( 5.68s )\n",
            "Costs of filter  59:   165 ( 5.67s )\n",
            "Costs of filter  61:   149 ( 5.71s )\n",
            "Costs of filter  62:    84 ( 5.70s )\n",
            "Costs of filter  63:   105 ( 5.74s )\n",
            "Costs of filter  64:   191 ( 5.75s )\n",
            "Costs of filter  65:    38 ( 5.74s )\n",
            "Costs of filter  70:    95 ( 5.91s )\n",
            "Costs of filter  74:    53 ( 5.77s )\n",
            "Costs of filter  75:    55 ( 5.77s )\n",
            "Costs of filter  76:   139 ( 5.80s )\n",
            "Costs of filter  79:   109 ( 5.78s )\n",
            "Costs of filter  80:    73 ( 5.85s )\n",
            "Costs of filter  81:    68 ( 5.83s )\n",
            "Costs of filter  82:    89 ( 5.86s )\n",
            "Costs of filter  84:    67 ( 5.90s )\n",
            "Costs of filter  86:   171 ( 5.84s )\n",
            "Costs of filter  87:   154 ( 5.87s )\n",
            "Costs of filter  88:   152 ( 5.91s )\n",
            "Costs of filter  91:    96 ( 5.89s )\n",
            "Costs of filter  93:   103 ( 5.94s )\n",
            "Costs of filter  95:    46 ( 5.96s )\n",
            "Costs of filter  97:    65 ( 5.97s )\n",
            "Costs of filter 101:    52 ( 6.12s )\n",
            "Costs of filter 102:   137 ( 6.20s )\n",
            "Costs of filter 103:   127 ( 6.02s )\n",
            "Costs of filter 108:    82 ( 6.05s )\n",
            "Costs of filter 109:    44 ( 6.10s )\n",
            "Costs of filter 110:    94 ( 6.10s )\n",
            "Costs of filter 112:    50 ( 6.11s )\n",
            "Costs of filter 113:   102 ( 6.13s )\n",
            "Costs of filter 114:   129 ( 6.13s )\n",
            "Costs of filter 119:   144 ( 6.33s )\n",
            "Costs of filter 121:    61 ( 6.18s )\n",
            "Costs of filter 124:    70 ( 6.23s )\n",
            "Costs of filter 126:   148 ( 6.20s )\n",
            "Costs of filter 127:    58 ( 6.24s )\n",
            "Costs of filter 128:   116 ( 6.26s )\n",
            "Costs of filter 131:    98 ( 6.25s )\n",
            "Costs of filter 132:    64 ( 6.30s )\n",
            "Costs of filter 133:    84 ( 6.28s )\n",
            "Costs of filter 135:    68 ( 6.26s )\n",
            "Costs of filter 136:   108 ( 6.29s )\n",
            "Costs of filter 137:    73 ( 6.32s )\n",
            "Costs of filter 138:   236 ( 6.38s )\n",
            "Costs of filter 139:    67 ( 6.53s )\n",
            "Costs of filter 140:    80 ( 6.35s )\n",
            "Costs of filter 141:    93 ( 6.34s )\n",
            "Costs of filter 143:   109 ( 6.34s )\n",
            "Costs of filter 145:    74 ( 6.38s )\n",
            "Costs of filter 146:   167 ( 6.44s )\n",
            "Costs of filter 149:    65 ( 6.59s )\n",
            "Costs of filter 152:   130 ( 6.47s )\n",
            "Costs of filter 154:   102 ( 6.49s )\n",
            "Costs of filter 155:    72 ( 6.50s )\n",
            "Costs of filter 157:   154 ( 6.52s )\n",
            "Costs of filter 158:   101 ( 6.53s )\n",
            "Costs of filter 161:    77 ( 6.69s )\n",
            "Costs of filter 162:    92 ( 6.55s )\n",
            "Costs of filter 165:    62 ( 6.60s )\n",
            "Costs of filter 166:   131 ( 6.57s )\n",
            "Costs of filter 167:   111 ( 6.62s )\n",
            "Costs of filter 168:    83 ( 6.64s )\n",
            "Costs of filter 170:   109 ( 6.66s )\n",
            "Costs of filter 171:    94 ( 6.69s )\n",
            "Costs of filter 173:   154 ( 6.82s )\n",
            "Costs of filter 174:   107 ( 6.69s )\n",
            "Costs of filter 175:    83 ( 6.70s )\n",
            "Costs of filter 177:   110 ( 6.74s )\n",
            "Costs of filter 180:    65 ( 6.74s )\n",
            "Costs of filter 181:    90 ( 6.78s )\n",
            "Costs of filter 182:   188 ( 6.77s )\n",
            "Costs of filter 185:   103 ( 6.97s )\n",
            "Costs of filter 186:    86 ( 7.08s )\n",
            "Costs of filter 187:   127 ( 6.86s )\n",
            "Costs of filter 188:    81 ( 6.87s )\n",
            "Costs of filter 189:    74 ( 6.90s )\n",
            "Costs of filter 190:    72 ( 6.89s )\n",
            "Costs of filter 192:    53 ( 6.92s )\n",
            "Costs of filter 201:    88 ( 7.14s )\n",
            "Costs of filter 202:    79 ( 7.03s )\n",
            "Costs of filter 203:   103 ( 6.99s )\n",
            "Costs of filter 204:    71 ( 7.03s )\n",
            "Costs of filter 206:    92 ( 7.09s )\n",
            "Costs of filter 207:    55 ( 7.15s )\n",
            "Costs of filter 208:   150 ( 7.08s )\n",
            "Costs of filter 209:   130 ( 7.09s )\n",
            "Costs of filter 210:   128 ( 7.09s )\n",
            "Costs of filter 211:    59 ( 7.13s )\n",
            "Costs of filter 213:   123 ( 7.14s )\n",
            "Costs of filter 214:    86 ( 7.14s )\n",
            "Costs of filter 218:    81 ( 7.36s )\n",
            "Costs of filter 219:   127 ( 7.21s )\n",
            "Costs of filter 220:   108 ( 7.23s )\n",
            "Costs of filter 221:    96 ( 7.18s )\n",
            "Costs of filter 223:    48 ( 7.21s )\n",
            "Costs of filter 227:    84 ( 7.48s )\n",
            "Costs of filter 233:    50 ( 7.32s )\n",
            "Costs of filter 235:   145 ( 7.37s )\n",
            "Costs of filter 236:   111 ( 7.43s )\n",
            "Costs of filter 237:    84 ( 7.45s )\n",
            "Costs of filter 238:    89 ( 7.46s )\n",
            "Costs of filter 240:    85 ( 7.54s )\n",
            "Costs of filter 241:   122 ( 7.52s )\n",
            "Costs of filter 243:   114 ( 7.53s )\n",
            "Costs of filter 245:   162 ( 7.51s )\n",
            "Costs of filter 246:    58 ( 7.51s )\n",
            "Costs of filter 247:    59 ( 7.55s )\n",
            "Costs of filter 249:    91 ( 7.73s )\n",
            "Costs of filter 250:   167 ( 7.57s )\n",
            "Costs of filter 251:    92 ( 7.62s )\n",
            "Costs of filter 255:    75 ( 7.63s )\n",
            "Costs of filter 256:    64 ( 7.65s )\n",
            "Costs of filter 257:    85 ( 7.68s )\n",
            "Costs of filter 258:    98 ( 7.69s )\n",
            "Costs of filter 262:    79 ( 7.82s )\n",
            "Costs of filter 263:    84 ( 7.80s )\n",
            "Costs of filter 264:   113 ( 7.73s )\n",
            "Costs of filter 265:   130 ( 7.80s )\n",
            "Costs of filter 267:    55 ( 7.74s )\n",
            "Costs of filter 268:    80 ( 7.92s )\n",
            "Costs of filter 269:    77 ( 7.81s )\n",
            "Costs of filter 271:    62 ( 7.82s )\n",
            "Costs of filter 272:   104 ( 7.88s )\n",
            "Costs of filter 275:    86 ( 7.87s )\n",
            "Costs of filter 278:   125 ( 8.08s )\n",
            "Costs of filter 280:    29 ( 7.74s )\n",
            "Costs of filter 281:    97 ( 7.96s )\n",
            "Costs of filter 282:    82 ( 7.96s )\n",
            "Costs of filter 285:    94 ( 8.36s )\n",
            "Costs of filter 286:   170 ( 7.89s )\n",
            "Costs of filter 287:    75 ( 8.01s )\n",
            "Costs of filter 289:    83 ( 8.06s )\n",
            "Costs of filter 293:    60 ( 8.09s )\n",
            "Costs of filter 294:   139 ( 8.30s )\n",
            "Costs of filter 295:   145 ( 8.16s )\n",
            "Costs of filter 296:    76 ( 8.16s )\n",
            "Costs of filter 297:   107 ( 8.20s )\n",
            "Costs of filter 299:    82 ( 8.22s )\n",
            "Costs of filter 300:   159 ( 8.23s )\n",
            "Costs of filter 301:   137 ( 8.41s )\n",
            "Costs of filter 302:   162 ( 8.26s )\n",
            "Costs of filter 303:    96 ( 8.26s )\n",
            "Costs of filter 305:    69 ( 8.26s )\n",
            "Costs of filter 308:   131 ( 8.34s )\n",
            "Costs of filter 309:   222 ( 8.41s )\n",
            "Costs of filter 311:    82 ( 8.39s )\n",
            "Costs of filter 320:   113 ( 8.56s )\n",
            "Costs of filter 322:   136 ( 8.47s )\n",
            "Costs of filter 323:    78 ( 8.50s )\n",
            "Costs of filter 327:    89 ( 8.58s )\n",
            "Costs of filter 328:   125 ( 8.75s )\n",
            "Costs of filter 331:    86 ( 8.63s )\n",
            "Costs of filter 333:    73 ( 8.57s )\n",
            "Costs of filter 336:    58 ( 8.70s )\n",
            "Costs of filter 339:    58 ( 8.86s )\n",
            "Costs of filter 341:    63 ( 8.66s )\n",
            "Costs of filter 345:    79 ( 8.82s )\n",
            "Costs of filter 346:   161 ( 9.08s )\n",
            "Costs of filter 348:   143 ( 8.92s )\n",
            "Costs of filter 349:    61 ( 8.92s )\n",
            "Costs of filter 352:    93 ( 8.92s )\n",
            "Costs of filter 353:    98 ( 9.13s )\n",
            "Costs of filter 356:    83 ( 8.99s )\n",
            "Costs of filter 357:    62 ( 8.99s )\n",
            "Costs of filter 359:    83 ( 9.03s )\n",
            "Costs of filter 360:    67 ( 9.21s )\n",
            "Costs of filter 361:    73 ( 9.08s )\n",
            "Costs of filter 365:   133 ( 9.11s )\n",
            "Costs of filter 366:    63 ( 9.20s )\n",
            "Costs of filter 367:   119 ( 9.27s )\n",
            "Costs of filter 368:   147 ( 9.19s )\n",
            "Costs of filter 371:   184 ( 9.21s )\n",
            "Costs of filter 372:   105 ( 9.27s )\n",
            "Costs of filter 374:    62 ( 9.46s )\n",
            "Costs of filter 375:    70 ( 9.34s )\n",
            "Costs of filter 376:   167 ( 9.33s )\n",
            "Costs of filter 377:   119 ( 9.34s )\n",
            "Costs of filter 378:    89 ( 9.20s )\n",
            "Costs of filter 380:    68 ( 9.48s )\n",
            "Costs of filter 381:    65 ( 9.37s )\n",
            "Costs of filter 383:   173 ( 9.44s )\n",
            "Costs of filter 387:    89 ( 9.62s )\n",
            "Costs of filter 389:    80 ( 9.46s )\n",
            "Costs of filter 394:    79 ( 9.73s )\n",
            "Costs of filter 397:   100 ( 9.65s )\n",
            "Costs of filter 399:   109 ( 9.64s )\n",
            "Costs of filter 402:    78 ( 9.77s )\n",
            "Costs of filter 405:   108 ( 9.69s )\n",
            "Costs of filter 410:    46 ( 9.89s )\n",
            "Costs of filter 412:   151 ( 9.81s )\n",
            "Costs of filter 414:    65 ( 9.83s )\n",
            "Costs of filter 415:   129 ( 9.86s )\n",
            "Costs of filter 417:   116 ( 9.93s )\n",
            "Costs of filter 418:   114 ( 9.93s )\n",
            "Costs of filter 421:    77 ( 9.96s )\n",
            "Costs of filter 422:    42 ( 10.00s )\n",
            "Costs of filter 423:   142 ( 10.19s )\n",
            "Costs of filter 424:   111 ( 9.99s )\n",
            "Costs of filter 426:    70 ( 10.07s )\n",
            "Costs of filter 427:   107 ( 10.04s )\n",
            "Costs of filter 428:   123 ( 10.31s )\n",
            "Costs of filter 429:    65 ( 9.91s )\n",
            "Costs of filter 430:    72 ( 10.10s )\n",
            "Costs of filter 433:   101 ( 10.12s )\n",
            "Costs of filter 435:    82 ( 10.40s )\n",
            "Costs of filter 436:   133 ( 10.24s )\n",
            "Costs of filter 437:   119 ( 10.24s )\n",
            "Costs of filter 438:    96 ( 10.31s )\n",
            "Costs of filter 439:   137 ( 10.48s )\n",
            "Costs of filter 442:   110 ( 10.28s )\n",
            "Costs of filter 443:    79 ( 10.31s )\n",
            "Costs of filter 444:   117 ( 10.34s )\n",
            "Costs of filter 445:    81 ( 10.52s )\n",
            "Costs of filter 446:   132 ( 10.38s )\n",
            "Costs of filter 448:    73 ( 10.37s )\n",
            "Costs of filter 449:    88 ( 10.37s )\n",
            "Costs of filter 452:    89 ( 10.65s )\n",
            "Costs of filter 453:    95 ( 10.45s )\n",
            "Costs of filter 454:    52 ( 10.53s )\n",
            "Costs of filter 455:   116 ( 10.64s )\n",
            "Costs of filter 457:    95 ( 10.56s )\n",
            "Costs of filter 458:    90 ( 10.45s )\n",
            "Costs of filter 460:   117 ( 10.55s )\n",
            "Costs of filter 461:    63 ( 10.79s )\n",
            "Costs of filter 462:    82 ( 10.66s )\n",
            "Costs of filter 463:    32 ( 10.66s )\n",
            "Costs of filter 464:    67 ( 10.48s )\n",
            "Costs of filter 465:    64 ( 10.85s )\n",
            "Costs of filter 467:    70 ( 10.72s )\n",
            "Costs of filter 473:   131 ( 10.99s )\n",
            "Costs of filter 474:    57 ( 10.81s )\n",
            "Costs of filter 475:    67 ( 10.83s )\n",
            "Costs of filter 476:    78 ( 10.79s )\n",
            "Costs of filter 481:    80 ( 11.04s )\n",
            "Costs of filter 482:    35 ( 10.90s )\n",
            "Costs of filter 483:    66 ( 11.13s )\n",
            "Costs of filter 484:    51 ( 11.06s )\n",
            "Costs of filter 485:   164 ( 11.09s )\n",
            "Costs of filter 487:    98 ( 11.13s )\n",
            "Costs of filter 489:    95 ( 11.09s )\n",
            "Costs of filter 490:   127 ( 11.16s )\n",
            "Costs of filter 493:    76 ( 11.05s )\n",
            "Costs of filter 494:   113 ( 11.35s )\n",
            "Costs of filter 495:    89 ( 11.21s )\n",
            "Costs of filter 496:    90 ( 11.25s )\n",
            "Costs of filter 499:   108 ( 11.49s )\n",
            "Costs of filter 500:   136 ( 11.18s )\n",
            "Costs of filter 501:   113 ( 11.34s )\n",
            "Costs of filter 502:    61 ( 11.45s )\n",
            "Costs of filter 503:    97 ( 11.58s )\n",
            "Costs of filter 505:   104 ( 11.42s )\n",
            "Costs of filter 506:    74 ( 11.46s )\n",
            "Costs of filter 509:    90 ( 11.74s )\n",
            "Costs of filter 510:   106 ( 11.51s )\n",
            "Costs of filter 511:   151 ( 11.51s )\n",
            "289 filter processed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}